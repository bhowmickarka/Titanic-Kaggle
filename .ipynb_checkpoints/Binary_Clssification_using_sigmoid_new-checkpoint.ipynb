{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the training dataset\n",
    "data_train=pd.read_csv('train.csv')\n",
    "data_test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Fare (0.25730652238496243, 6.120189341921873e-15)\n",
      "With Age (-0.06347428032868789, 0.05823382060578777)\n"
     ]
    }
   ],
   "source": [
    "#Trying some data Visulaization Techniques\n",
    "from scipy.stats import pearsonr #Importing the Pearson Correlation Coefficient calc\n",
    "print(\"With Fare\",pearsonr(data_train[\"Survived\"].values,data_train[\"Fare\"].values ))\n",
    "print(\"With Age\",pearsonr(data_train[\"Survived\"].values,data_train[\"Age\"].values ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Q'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-55ab8a49b5f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbest_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdfscores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \"\"\"\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    745\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    748\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Q'"
     ]
    }
   ],
   "source": [
    "#apply SelectKBest class to extract top  features\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X=data_train.iloc[:,3:]\n",
    "Y=data_train.iloc[:,2:3]\n",
    "best_features=SelectKBest(score_func=chi2, k=3)\n",
    "fit=best_features.fit(X,Y)\n",
    "dfscores=pd.DataFrame(fit.scores_)\n",
    "df.columns=pd.DataFrame(X.columns)\n",
    "\n",
    "print(\"DFscores\",dfscores)\n",
    "print(\"DFcolumns\",dfcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing train data\n",
    "data_train=data_train.drop(labels=[\"Name\",\"Ticket\",\"Cabin\",\"Embarked\"],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing Sex with numbers\n",
    "gender = {'male': 0,'female': 1}#Gender dictionary\n",
    "data_train.Sex=[gender[item] for item in data_train.Sex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using sklearn's LabelEncoder to perform one_hot_encoding in Pclass\n",
    "from sklearn import preprocessing\n",
    "onehot=preprocessing.LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying one hot encoding using Pandas\n",
    "data_train1=pd.get_dummies(data_train[\"Pclass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.drop(labels=['Pclass'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.join(data_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling up Nan in age\n",
    "data_train['Age']=data_train['Age'].fillna(method='ffill',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_features=data_train.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 7)\n"
     ]
    }
   ],
   "source": [
    "print(Train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_label=data_train.iloc[:,2:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ANACONDA\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#One Hot Encoding the labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oneHot=OneHotEncoder()\n",
    "oneHot.fit(Train_label) \n",
    "Train_Label = oneHot.transform(Train_label).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Train_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(Train_features,Train_Label,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 596 2\n"
     ]
    }
   ],
   "source": [
    "#Now defining the model\n",
    "M=X_train.shape[1]\n",
    "N=X_train.shape[0]\n",
    "\n",
    "LC=Y_train.shape[1]\n",
    "print(M,N,LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the inputs of the Neural Network\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,M])\n",
    "Y=tf.placeholder(tf.float32,[None,LC])\n",
    "epochs=3000\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#Defining the model \n",
    "initializer=tf.contrib.layers.xavier_initializer()#Initializing the weight matrix\n",
    "h0=tf.layers.dense(X,units=5,activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "h1=tf.layers.dense(h0,units=LC,activation=tf.nn.sigmoid)\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=h1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "predicted=tf.nn.sigmoid(h1)\n",
    "correct_pred=tf.equal(tf.round(predicted),Y)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 0.694\tAcc: 50.00%\n",
      "Step:   100\tLoss: 0.610\tAcc: 50.00%\n",
      "Step:   200\tLoss: 0.599\tAcc: 50.00%\n",
      "Step:   300\tLoss: 0.593\tAcc: 50.00%\n",
      "Step:   400\tLoss: 0.591\tAcc: 50.00%\n",
      "Step:   500\tLoss: 0.591\tAcc: 50.00%\n",
      "Step:   600\tLoss: 0.590\tAcc: 50.00%\n",
      "Step:   700\tLoss: 0.590\tAcc: 50.25%\n",
      "Step:   800\tLoss: 0.589\tAcc: 51.17%\n",
      "Step:   900\tLoss: 0.589\tAcc: 51.76%\n",
      "Step:  1000\tLoss: 0.589\tAcc: 53.36%\n",
      "Step:  1100\tLoss: 0.588\tAcc: 54.61%\n",
      "Step:  1200\tLoss: 0.588\tAcc: 56.12%\n",
      "Step:  1300\tLoss: 0.588\tAcc: 58.14%\n",
      "Step:  1400\tLoss: 0.588\tAcc: 60.82%\n",
      "Step:  1500\tLoss: 0.587\tAcc: 61.91%\n",
      "Step:  1600\tLoss: 0.587\tAcc: 63.17%\n",
      "Step:  1700\tLoss: 0.587\tAcc: 63.67%\n",
      "Step:  1800\tLoss: 0.587\tAcc: 63.84%\n",
      "Step:  1900\tLoss: 0.587\tAcc: 64.77%\n",
      "Step:  2000\tLoss: 0.587\tAcc: 65.02%\n",
      "Step:  2100\tLoss: 0.587\tAcc: 65.27%\n",
      "Step:  2200\tLoss: 0.587\tAcc: 65.44%\n",
      "Step:  2300\tLoss: 0.586\tAcc: 65.52%\n",
      "Step:  2400\tLoss: 0.586\tAcc: 65.60%\n",
      "Step:  2500\tLoss: 0.586\tAcc: 65.86%\n",
      "Step:  2600\tLoss: 0.586\tAcc: 65.86%\n",
      "Step:  2700\tLoss: 0.586\tAcc: 66.11%\n",
      "Step:  2800\tLoss: 0.586\tAcc: 66.28%\n",
      "Step:  2900\tLoss: 0.586\tAcc: 66.36%\n",
      "Step:  3000\tLoss: 0.586\tAcc: 66.28%\n",
      "Step:  3100\tLoss: 0.586\tAcc: 66.28%\n",
      "Step:  3200\tLoss: 0.586\tAcc: 66.28%\n",
      "Step:  3300\tLoss: 0.586\tAcc: 66.28%\n",
      "Step:  3400\tLoss: 0.586\tAcc: 66.28%\n",
      "Step:  3500\tLoss: 0.586\tAcc: 66.53%\n",
      "Step:  3600\tLoss: 0.586\tAcc: 66.61%\n",
      "Step:  3700\tLoss: 0.586\tAcc: 66.69%\n",
      "Step:  3800\tLoss: 0.586\tAcc: 66.78%\n",
      "Step:  3900\tLoss: 0.586\tAcc: 66.78%\n",
      "Step:  4000\tLoss: 0.586\tAcc: 66.78%\n",
      "Step:  4100\tLoss: 0.586\tAcc: 66.78%\n",
      "Step:  4200\tLoss: 0.586\tAcc: 66.95%\n",
      "Step:  4300\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  4400\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  4500\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  4600\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  4700\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  4800\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  4900\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  5000\tLoss: 0.586\tAcc: 67.03%\n",
      "Step:  5100\tLoss: 0.586\tAcc: 67.20%\n",
      "Step:  5200\tLoss: 0.586\tAcc: 67.20%\n",
      "Step:  5300\tLoss: 0.586\tAcc: 67.20%\n",
      "Step:  5400\tLoss: 0.586\tAcc: 67.20%\n",
      "Step:  5500\tLoss: 0.586\tAcc: 67.20%\n",
      "Step:  5600\tLoss: 0.586\tAcc: 67.20%\n",
      "Step:  5700\tLoss: 0.586\tAcc: 67.20%\n",
      "Step:  5800\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  5900\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6000\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6100\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6200\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6300\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6400\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6500\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6600\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6700\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6800\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  6900\tLoss: 0.586\tAcc: 67.37%\n",
      "Step:  7000\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7100\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7200\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7300\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7400\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7500\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7600\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7700\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7800\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  7900\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8000\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8100\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8200\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8300\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8400\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8500\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8600\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8700\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  8800\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  8900\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  9000\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  9100\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  9200\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  9300\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  9400\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  9500\tLoss: 0.585\tAcc: 67.28%\n",
      "Step:  9600\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  9700\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  9800\tLoss: 0.585\tAcc: 67.37%\n",
      "Step:  9900\tLoss: 0.585\tAcc: 67.37%\n"
     ]
    }
   ],
   "source": [
    "#Creating the session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range (10000):\n",
    "        loss,_,acc=sess.run([cost ,optimizer , accuracy], feed_dict={X:X_train, Y:Y_train})\n",
    "        cost_history=np.append(cost_history,acc)\n",
    "        if epoch%100==0:\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(epoch, loss, acc))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
